# Dual-agent AI coding workflows in 2025–2026: the definitive practitioner guide

**Claude Opus 4.6 as implementer and GPT-5.3 Codex as adversarial reviewer is an emerging but validated pattern.** A 30-year software veteran documented this exact workflow — Claude writes code, Codex reviews, neither moves forward until both agree — and got a PR merged into a 15,000-star open-source project after three rounds of maintainer feedback. The tooling now exists: VS Code v1.109 natively runs Claude, Codex, and Copilot side-by-side, GitHub Agent HQ lets you assign issues to multiple agents simultaneously, and MCP bridges enable direct Claude→Codex communication. But the pattern carries serious risks: the METR randomized controlled trial found developers *believe* AI makes them 20% faster while actually being **19% slower**, and dual-agent workflows consume 7–15× more tokens than single-agent sessions. This report synthesizes practitioner reports, official documentation, framework comparisons, and documented failure modes to map the current state of the art.

---

## The tools are real and the ecosystem has converged

Both model versions in the user's query exist and shipped within weeks of each other. **Claude Opus 4.6** launched February 5, 2026 with 1M-token context (beta), agent teams, and a METR 50%-time horizon of 14 hours 30 minutes, priced at $5/$25 per million input/output tokens. **GPT-5.3-Codex** arrived the same month, 25% faster than its predecessor, available across all Codex surfaces including the VS Code extension, CLI, desktop app, and GitHub Agent HQ. **Google Antigravity IDE** — announced November 18, 2025 alongside Gemini 3 — is a heavily modified VS Code fork with a "Manager View" for orchestrating multiple agents in parallel, supporting Claude Opus 4.6, Claude Sonnet 4.6, and GPT models natively.

The configuration ecosystem has bifurcated into two complementary standards. **CLAUDE.md** is Anthropic-specific, loaded as part of Claude Code's system prompt at session start, with a hierarchy from global (`~/.claude/CLAUDE.md`) through project root to subdirectories. **AGENTS.md** is the cross-tool standard maintained by the Agentic AI Foundation under the Linux Foundation, supported by over 20 tools including Codex, Cursor, Google Jules, GitHub Copilot, Gemini CLI, and Windsurf. GitHub's analysis of 2,500+ AGENTS.md files found the most effective ones stay under **150 lines**, put build commands early, use code examples over explanations, and set explicit boundaries. The recommended approach for dual-agent workflows: use AGENTS.md for universal instructions and CLAUDE.md with an `@AGENTS.md` import for Claude-specific features like subagents and hooks.

For TDD enforcement specifically, Claude Code's hooks system is the most powerful mechanism available — **15 hook events and 3 handler types** (command, prompt, and agent hooks) enable mandatory test-first cycles. The open-source **TDD Guard** project blocks file modifications if no corresponding failing test exists, supporting Jest, Vitest, pytest, and more. On the Codex side, automatic PR reviews driven by AGENTS.md guidelines can be enabled in settings, with GPT-5.2-Codex specifically trained for code review accuracy.

---

## Community practices reveal a clear "Claude builds, Codex validates" consensus

The practitioner evidence strongly favors Claude as implementer and Codex as reviewer, not the reverse. **ShakaCode's guide** documents the exact pattern: Claude Code implements a feature in Session 1, then Codex reviews the branch for bugs, security issues, and test coverage in Session 2. Builder.io CEO Steve Sewell's production comparison found Claude Code's GitHub reviews were "verbose without catching obvious bugs" while Codex's GitHub bot "actually finds legitimate, hard-to-spot bugs, comments inline." His recommendation: "Design architecture and run deep investigations with Claude Code, then pass focused implementation tasks to Codex for fast execution."

Simon Willison, creator of Datasette, documented his daily workflow in October 2025: multiple terminal windows with Claude Code (Sonnet 4.5) and Codex CLI running in different directories using git worktrees for isolation, noting the bottleneck is **human review speed**, not agent generation speed. Reddit analysis aggregating r/ClaudeCode, r/ChatGPTCoding, and r/ClaudeAI found that 65.3% of direct comparison comments preferred Codex (79.9% when weighted by upvotes), though Claude generated 4× more discussion volume. The emerging Reddit consensus frames it as "Claude as the big-picture planner and Codex as the shell-first surgeon."

The most detailed first-person report comes from a developer (Sakiharu) who spent two years building a structured dual-agent workflow. Their key insight: "The efficiency jumped immediately. Not because the agents got smarter, but because the review discipline became automatic instead of depending on my willpower at 2am." They discovered characteristic agent failure patterns: **Claude Code skips error handling when context gets long; Codex over-engineers abstractions but catches edge cases Claude misses.** Crucially, they warn: "Two AIs can happily agree on a bad design. Without domain judgment from a human, this is just two agents rubber-stamping each other. The human arbiter is not optional."

No official guidance exists from either Anthropic or OpenAI about using their tool alongside the competitor's. The closest is GitHub Agent HQ (launched at Universe, November 2025), which explicitly enables assigning issues to Copilot, Claude, and Codex simultaneously to compare approaches. OpenAI's "Use Codex with the Agents SDK" guide enables exposing Codex as an MCP server for multi-agent pipelines, and Anthropic's subagent and agent teams documentation supports internal multi-agent patterns.

---

## The handoff problem remains unsolved but three patterns lead

The handoff between implementation and review agents is the critical engineering challenge, and **no universal standard exists** as of February 2026. XTrace articulates the core dilemma: dump everything (40,000 tokens of process to find 3,000 tokens of insight) and drown the receiving agent, or compress to a summary and strip away evidence and reasoning. Three patterns have emerged as frontrunners.

**Pattern 1: Structured markdown with YAML frontmatter.** This is the most practical approach for the Claude→Codex handoff. GitHub's Spec Kit (v0.1.4, February 2026) formalizes this as spec.md → plan.md → tasks.md living in a `.specify` directory. For dual-agent review, the handoff artifact includes machine-parseable YAML frontmatter (verdict, files changed, test status, iteration count) and a markdown body for reasoning, findings, and file references. Claude Code users have independently developed session handoff documents saved as `.claude/handoff-[date].md` with sections for accomplished work, current state, next steps, and important context.

**Pattern 2: MCP-based bridging.** This is the most technically elegant approach and works today. Codex CLI can run as an MCP server (`codex mcp-server`), and Claude Desktop can connect to it directly. Dedicated bridging tools include **Codex Bridge** (consultation and batch processing between Claude and Codex), **PAL MCP Server** (spawning cross-agent subagents — "Claude Code can spawn Codex subagents"), and **Context Pack** (an MCP tool where sub-agents place lightweight "anchors" instead of copying code, with the orchestrator rendering all anchors into a single markdown pack on demand). The MCP approach keeps handoff artifacts small while enabling rich context retrieval on demand.

**Pattern 3: Git/PR-based handoff.** This is the most mature pattern, leveraging existing infrastructure. Agent A creates code, pushes a branch, opens a PR; Agent B reviews inline. GitHub Agentic Workflows (technical preview, February 2026) formalize this as agent-neutral CI/CD where implementation and review agents are orchestrated in markdown-authored workflows. Tools like Qodo/PR-Agent, CodeRabbit, and Graphite Agent all operate on this model, with Graphite achieving a ~90-second feedback loop (40× faster than human review).

On context size, the research converges on a clear recommendation: **diffs plus architectural context, not full files**. Qodo 2.0's multi-agent review architecture runs an "alignment phase" synthesizing PR description, linked tickets, repository-level patterns, and cross-file dependency graphs into a structured context object *before* code analysis begins. Martin Fowler's team at Thoughtworks notes that even with 1M-token context windows, "it doesn't mean it's a good idea to indiscriminately dump information in there." LangChain's token analysis shows the subagent pattern uses **67% fewer tokens** (9K vs 14K+) compared to handoff patterns with growing conversation history.

The implementation agent's reasoning **should** be included in the handoff. Multiple sources confirm this: Qodo 2.0 includes "evidence and reasoning that explains how the agent reached its conclusion" in every review finding. Without reasoning, the reviewer is reduced to a smart linter over diffs — the 2023-2024 era approach that failed to catch architectural issues. A Qodo example illustrates this: removing 23 lines looked like cleanup, passed tests, didn't break the build, but architectural context revealed it was a critical RBAC security change.

---

## Automation infrastructure has matured dramatically since mid-2025

VS Code v1.109 (February 2026) is now the canonical multi-agent coding platform. Claude, Codex, and Copilot run simultaneously in a unified Agent Sessions view, with both local and cloud execution options. Developers can delegate different tasks to different agents, compare outputs, and orchestrate handoffs from Plan → Implement → Review all within one workspace. Claude Code's extension (`anthropic.claude-code`) provides subagents, hooks, and inline diffs; Codex's extension (`openai.chatgpt`) offers three modes from chat-only planning through full autonomous access.

For CI/CD integration, both companies ship official GitHub Actions. **`openai/codex-action@v1`** installs Codex CLI, runs code reviews, and posts results as PR comments with configurable safety strategies (drop-sudo, read-only, unprivileged-user). **`anthropics/claude-code-action`** enables PR-centric workflows including review comments, test generation, and refactoring proposals. GitHub's new **Agentic Workflows** (technical preview) combine these into agent-neutral CI/CD pipelines authored in markdown: six pattern categories covering Continuous Triage, Documentation, Code Simplification, Test Improvement, Quality Hygiene, and Reporting. Critically, PRs are never merged automatically — humans always make the final call.

The orchestration framework landscape offers several options for more complex pipelines:

- **OpenAI Agents SDK + Codex MCP** provides the most direct path: run Codex as an MCP server, connect from agents built with the SDK, chain PM → Designer → Developer → Tester with traces dashboard capturing every prompt and handoff
- **LangGraph** offers graph-based orchestration with conditional routing, fan-out/fan-in parallelism, and `interrupt()` for human-in-the-loop — best suited for complex coding workflows with branching logic
- **OpenHands** (formerly OpenDevin, 64K+ GitHub stars) supports multi-agent collaboration via `AgentDelegateAction` with sandboxed Docker execution and 72% resolution on SWE-Bench Verified
- **Ruflo** provides dual-mode initialization creating both CLAUDE.md and AGENTS.md, with swarm templates for architect → coder → tester → reviewer pipelines

For the specific Claude→Codex TDD workflow, the most practical automation is a shell script pattern that chains Claude Code's non-interactive mode (`claude -p`) with Codex's headless mode (`codex exec --json`), using git commits as handoff points. Claude Code's hooks system can enforce the TDD cycle (PreToolUse hooks block edits without failing tests), while Codex's `--output-schema` flag produces machine-readable review verdicts that feed back into the next Claude session.

---

## The real developer experience: powerful but cognitively expensive

The subjective experience of dual-agent coding varies sharply by task complexity and developer experience. The Sakiharu report describes it as closer to being a tech lead than a pair programmer: "You sit in the middle, Claude Code writes, Codex reviews, and you make the judgment calls." One significant advantage is **complementary context windows** — "When one agent's context is compressed and loses details, the other agent still remembers. They don't share the same context window, so they don't lose the same information at the same time."

However, managing multiple agents creates substantial overhead. A developer who built "Parallel Code" (a desktop app for managing AI agent sessions) noted that "running multiple sessions in parallel quickly becomes chaotic," requiring each session to be treated like an isolated feature branch. Token costs compound rapidly: agent teams in Claude Code use approximately **7× more tokens** than standard sessions, and multi-agent systems generally consume 15× more tokens due to looping, re-evaluation, and inter-agent communication. At API rates, a heavy dual-agent user could spend **$200–400+ per month** on subscriptions alone, or vastly more — one case study documented $5,623 in a single month at API rates for 201 Claude sessions.

The METR randomized controlled trial delivers a sobering counterpoint to productivity claims. Sixteen experienced open-source developers completed 246 real-world tasks on mature repositories (averaging 22,000+ stars, 1M+ lines). They predicted AI would make them 24% faster. After using AI, they believed they were 20% faster. **Actual measured result: 19% slower.** The DORA report corroborated this: every 25% increase in AI adoption showed a 1.5% dip in delivery speed and **7.2% drop in system stability**. A key caveat: this studied primarily Cursor Pro with Claude 3.5/3.7 Sonnet on complex, mature codebases — the newer agent-first tools and models may perform differently. The Stack Overflow 2025 survey found 78% of developers believe AI improves productivity, but trust in AI accuracy dropped from 43% to **33%** year-over-year, and 45% report debugging AI-generated code takes more time than expected.

---

## Nine failure modes that will destroy your dual-agent workflow

The failure modes of dual-agent coding workflows are well-documented and severe. Understanding them is essential before adopting this pattern.

**Rubber-stamp reviewing** is the most insidious risk. Sean Goedecke warns that the trust patterns developers develop with competent human colleagues — glancing at diffs, clicking approve — break catastrophically with AI agents. The "CI Amplification effect" makes this worse: each green checkmark from automated checks becomes another excuse to trust without thinking, widening the gap between formal approval and actual understanding. Tahir Yamin describes the "responsibility vacuum" in scaled systems: "AI coding agent submits PRs across fifty repositories. Every PR passes CI/CD. Green checkmarks everywhere. Human reviewers glance at diffs for twenty seconds each."

**TDD theater** is particularly dangerous in test-first workflows. David Adamo Jr. explains the mechanism: "When you ask an AI to generate tests, it almost always starts by analyzing the code you're asking it to test. Your tests now validate the implementation, not the intention." A DEV Community case study documented a team where AI-generated tests validated mocked setups rather than behavioral contracts, but coverage metrics rose, creating false confidence. The mitigation is writing tests *before* implementation (true TDD, not retrofit testing) and having the review agent specifically check whether tests encode intent rather than mirror implementation.

**Context rot** degrades agent performance as sessions lengthen. Research from Chroma found models "do not use their context uniformly; performance grows increasingly unreliable as input length grows." Drew Breunig identified context poisoning (hallucinated state infecting future decisions) and the "lost in the middle" phenomenon. Beyond **100,000 tokens**, agents show a "tendency toward favoring repeating actions from history rather than synthesizing novel plans." For dual-agent workflows, this means handoff artifacts must be kept lean and context windows should be reset between iterations.

**Agent ping-pong** occurs when fix-review cycles fail to converge. The Sakiharu report documents this: "Sometimes the agents ping-pong — a fix introduces a new issue, review catches it, the next fix introduces another issue." Research shows iterative code generation can actually *increase* vulnerabilities — GPT-4o showed a **37.6% increase in critical vulnerabilities after only five iterations**. Hard iteration limits (3–5 rounds) with human escalation are essential.

**Columbia DAPLab's systematic analysis** of 15+ vibe-coded applications identified nine critical failure patterns, with three key findings: agents prioritize runnable code over correctness, struggle with business logic regardless of model capability, and make increasingly more failures as codebase size grows. Anthropic's own engineering blog acknowledges this: "Even a frontier coding model like Opus 4.5 running in a loop across multiple context windows will fall short of building a production-quality web app if it's only given a high-level prompt."

**Cost spirals** are real. Dual-model workflows roughly double token consumption before accounting for the 7–15× multiplier from agentic patterns. An enterprise case study cited $4,000/month ($2,000 Claude API + $1,500 OpenAI + $500 monitoring). The mitigation is a portfolio approach: cheaper models (Haiku, codex-mini) for routine tasks, frontier models only for complex work.

---

## The recommended architecture for a production dual-agent TDD workflow

Based on the synthesis of practitioner reports, official documentation, and documented failure modes, here is the optimal architecture for the Claude Opus 4.6 → GPT-5.3 Codex dual-agent TDD workflow in a Python/TypeScript monorepo.

**Handoff artifact: structured markdown with YAML frontmatter, targeting 2,000–5,000 tokens.** The artifact should include: task description with acceptance criteria, git diff reference (not inline code), test results with pass/fail counts, implementation reasoning (2–3 sentences per major decision), known gotchas, and remaining work. Use YAML frontmatter for machine-parseable fields (verdict, iteration count, files changed, test status) and markdown body for human-readable context. Store artifacts in `.handoffs/[date]-[task-id].md` and reference them from the next agent's prompt.

**Agent configuration uses both standards.** Create an AGENTS.md (under 150 lines) at project root with build commands, test commands, code style, project structure, and boundaries — this serves both agents. Create a CLAUDE.md that imports AGENTS.md and adds Claude-specific instructions: TDD enforcement rules ("YOU MUST write failing tests before implementation"), subagent definitions in `.claude/agents/` for a dedicated reviewer agent with read-only tools, and hooks for pre-commit test validation. Configure Codex via `.codex/config.toml` with `project_doc_fallback_filenames = ["CLAUDE.md", "COPILOT.md"]` and a dedicated review section in AGENTS.md specifying: "YOU MUST NOT modify source code. Run the full test suite. Report findings with file:line references and severity levels."

**The feedback loop follows a maker-checker pattern with hard limits.** The review agent produces a structured verdict: `approved`, `changes_required`, or `rejected`, with specific findings at file:line granularity, severity levels (critical/major/minor/nit), and suggested fixes. This feeds back to the implementation agent as part of its next prompt context. **Maximum 3–5 review iterations** before human escalation. Track iteration count in the handoff artifact's YAML frontmatter. Convergence criteria: all tests pass AND no critical findings AND diff between iterations drops below a meaningful threshold.

**Work granularity maps to TDD cycles.** The optimal unit is a **single TDD cycle completion** (failing test → minimal passing implementation → refactor) for tight feedback loops, or a batch of 3–5 related cycles for PR-sized chunks. Augment Code's data shows AI models achieve 87.2% Pass@1 on single-function benchmarks but only **19.36% on multi-file infrastructure tasks** — a 68-percentage-point degradation. Keep each handoff focused on one coherent behavioral change.

**Automation level: semi-automated with human checkpoints at three gates.** Gate 1: human approves the implementation plan before Claude starts coding. Gate 2: human reviews the final handoff artifact after Codex approves (or when iteration limit is reached without convergence). Gate 3: human approves the merge to main. Between gates, the agents operate autonomously. Anthropic's own research shows engineers can "fully delegate" only 0–20% of tasks, with delegation reserved for easily verifiable, low-stakes work. Start with more human oversight and reduce as confidence builds.

**For full automation of the handoff itself**, chain Claude Code's non-interactive mode (`claude -p "implement feature X using TDD" --output-format json`) with Codex's headless review (`codex exec "review the changes on this branch" --json --output-last-message`), triggered by git hooks on branch push. The `openai/codex-action@v1` GitHub Action handles the CI/CD integration. For richer orchestration, expose Codex as an MCP server and let Claude Code call it directly — the PAL MCP Server and Codex Bridge tools support this today.

---

## Conclusion: a valid pattern with non-obvious risks

The dual-agent TDD workflow is technically feasible and supported by production tooling as of February 2026. VS Code, GitHub Agent HQ, MCP bridges, and official CI/CD actions provide the infrastructure. The "Claude builds, Codex reviews" division of labor is backed by practitioner evidence and plays to each model's documented strengths. But three insights from this research should temper enthusiasm.

First, **the human orchestrator is not optional** — they are the most important component. Every successful practitioner report emphasizes the human as tech lead making architectural decisions, breaking the agent ping-pong cycle, and catching the cases where both agents confidently agree on a wrong design. The dual-agent workflow is best understood not as replacing human judgment but as making the review discipline automatic.

Second, **the handoff artifact is the bottleneck, not the models.** The difference between a productive dual-agent workflow and an expensive token furnace comes down to what gets passed between agents. Include reasoning and architectural context, not just diffs. Keep artifacts under 5,000 tokens. Use MCP anchors or git references instead of inline code. Reset context between iterations.

Third, **TDD theater is the specific risk that makes this pattern dangerous.** When Claude writes both the tests and the implementation, the tests tend to validate the implementation rather than the intent — and Codex reviewing those tests has no independent access to the *intended* behavior unless it's explicitly encoded in the handoff. The mitigation is encoding acceptance criteria in natural language in the handoff artifact, separate from the test code, so the review agent can verify tests against intent rather than just checking that tests pass.