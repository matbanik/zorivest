# Dual-Agent Coding Workflow: Composite Research Synthesis

> **Sources**: ChatGPT Deep Research, Claude Research (Opus 4.6 + Web Search), Gemini Deep Research
> **Date**: 2026-02-28
> **Scope**: Claude Opus 4.6 (implementation) + GPT 5.3 Codex (validation) â€” community practices, tooling, pitfalls
> **Legend**: ðŸŸ¢ CONSENSUS (all 3 agree) Â· ðŸ”µ CONFIRMED (2/3 agree) Â· âšª UNIQUE (1 source)

---

## 1. Executive Summary

All three research reports independently converge on the same core finding: **"Claude builds, Codex validates" is the dominant dual-agent pattern in the 2026 developer community**, supported by production tooling (VS Code v1.109, MCP bridges, GitHub Actions) and practitioner evidence. The pattern works because Opus 4.6 excels at deep architectural reasoning and greenfield development (1M-token context, extended planning), while Codex 5.3 excels at terminal-native execution, aggressive codebase exploration, and precise code review (77.3% Terminal-Bench 2.0 vs Opus's 65.4%).

However, all three reports also agree on a critical caveat: **the human orchestrator is not optional**. The METR randomized controlled trial found developers *believe* AI makes them 20% faster while *actually being 19% slower* (on mature codebases with older models). Two AIs can confidently agree on a bad design. The dual-agent workflow is best understood not as replacing human judgment but as making review discipline automatic.

**Three universal warnings**:
1. **TDD theater** â€” AI-generated tests validate implementations, not intentions
2. **Rubber-stamp reviews** â€” Codex approving everything without real scrutiny
3. **Handoff quality** â€” the artifact passed between agents is the bottleneck, not the models

---

## 2. Agent Strengths & Role Assignment

### ðŸŸ¢ CONSENSUS: Opus = Architect/Builder, Codex = Reviewer/Validator

| Capability | Opus 4.6 | Codex 5.3 | Source |
|------------|----------|-----------|--------|
| Deep architectural reasoning | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† | All 3 |
| 1M-token context | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† | Claude, Gemini |
| Terminal-Bench 2.0 | 65.4% | 77.3% | Gemini |
| Token generation speed | Slower (deliberate) | ~240 tok/s (25% faster than predecessor) | Gemini |
| Edge case detection | Misses when context is long | Catches Opus misses | ChatGPT, Claude |
| Error handling | Skips under long context | Over-engineers but thorough | Claude |
| Code review quality | "Verbose without catching bugs" | "Finds legitimate, hard-to-spot bugs" | Claude (Builder.io CEO) |
| Failure mode | "Opus Slop" â€” bypassed edge cases, type loopholes | Over-engineering â€” 80K lines of pedantic tests | Gemini |

### ðŸ”µ CONFIRMED: Reddit Community Preference

- 65.3% of direct comparison comments preferred Codex (79.9% upvote-weighted) â€” Claude
- Claude generated 4Ã— more discussion volume â€” Claude
- Consensus framing: "Claude as big-picture planner, Codex as shell-first surgeon" â€” Claude

### ðŸ”µ CONFIRMED: Builder.io CEO (Steve Sewell) Recommendation

> "Design architecture and run deep investigations with Claude Code, then pass focused implementation tasks to Codex for fast execution." â€” Claude, Gemini

---

## 3. Workflow Patterns

### ðŸŸ¢ CONSENSUS: Standard Session Flow

All three reports describe essentially the same flow:

```
1. PLAN      Human scopes task, provides requirement
2. SPEC      Opus reads repo context, generates implementation plan
3. IMPLEMENT Opus writes tests + code (TDD), pushes rapid iterations
4. CHECKPOINT State saved â€” git commit + handoff artifact created
5. VALIDATE  Codex reads handoff + changed files, runs tests, reviews
6. FEEDBACK  Codex issues verdict â€” approved / changes_required
7. ITERATE   If changes_required, feedback routes back to Opus (max 3 rounds)
8. MERGE     Human approves final result
```

### ðŸŸ¢ CONSENSUS: Git Worktree Isolation

All three reports emphasize isolating each agent in its own git branch/worktree:
- Prevents agents from overwriting each other's work
- Primary branch stays pristine until validation completes
- Enables parallel agent execution without collision

### ðŸ”µ CONFIRMED: Multi-Session Orchestration

- Multi-agent orchestrator (initially Bash/AppleScript, later TypeScript) manages many Claude and Codex sessions in parallel â€” ChatGPT
- Simon Willison: multiple terminal windows with Claude Code and Codex CLI in different directories using git worktrees â€” Claude
- "Parallel Code" desktop app for managing AI agent sessions â€” Claude

### âšª UNIQUE: Traycer Planning Tool

- Traycer drafts a plan, Claude implements, Codex reviews against original plan â€” ChatGPT
- Creates file-level to-do lists so agents only see what they need â€” ChatGPT

---

## 4. Handoff Engineering

### ðŸŸ¢ CONSENSUS: Handoff Is the Critical Bottleneck

> "The difference between a productive dual-agent workflow and an expensive token furnace comes down to what gets passed between agents." â€” Claude

### ðŸ”µ CONFIRMED: Three Leading Handoff Patterns

| Pattern | Mechanism | Maturity | Best For |
|---------|-----------|----------|----------|
| **Structured Markdown + YAML frontmatter** | Files in `.handoffs/` or `.specify/` | Most practical | Claudeâ†’Codex handoff |
| **MCP-based bridging** | Direct agent-to-agent communication | Most elegant | Real-time review |
| **Git/PR-based** | Branch â†’ PR â†’ inline review | Most mature | CI/CD integration |

#### Pattern 1: Structured Markdown (Recommended)

- GitHub Spec Kit v0.1.4 formalizes: `spec.md â†’ plan.md â†’ tasks.md` in `.specify/` directory â€” Claude
- YAML frontmatter for machine-parseable fields (verdict, iteration count, files changed, test status) â€” Claude
- Markdown body for human-readable reasoning, findings, file references â€” Claude
- Claude Code users independently developed `.claude/handoff-[date].md` â€” Claude

#### Pattern 2: MCP Bridges

- **Codex Bridge** â€” consultation and batch processing between Claude and Codex â€” Claude
- **PAL MCP Server** â€” Claude Code spawns Codex subagents â€” Claude, Gemini
- **claude-codex-bridge** â€” MCP server for direct communication â€” Gemini
- **Context Pack** â€” lightweight "anchors" instead of copying code â€” Claude
- Anti-recursion guards via `BRIDGE_DEPTH` environment variable â€” Gemini

#### Pattern 3: Git/PR-based

- Graphite Agent: ~90-second feedback loop, 40Ã— faster than human review â€” Claude
- GitHub Agentic Workflows (technical preview, Feb 2026): agent-neutral CI/CD â€” Claude
- Qodo/PR-Agent, CodeRabbit for automated PR review â€” Claude

### ðŸŸ¢ CONSENSUS: Optimal Handoff Size and Content

| Dimension | Recommendation | Sources |
|-----------|---------------|---------|
| **Target size** | 2,000â€“5,000 tokens | Claude |
| **Include reasoning** | YES â€” without it, reviewer becomes a smart linter | Claude, ChatGPT |
| **Include diffs** | Reference diffs, don't inline full files | Claude, Gemini |
| **Include acceptance criteria** | YES â€” so reviewer can verify tests against intent | Claude |
| **Format** | Machine-parseable YAML frontmatter + human-readable markdown | Claude, ChatGPT |
| **JSON verdict format** | `{verdict, issues, suggestions}` â€” strict, no extra text | ChatGPT |

### ðŸŸ¢ CONSENSUS: Include Implementation Reasoning

> "Without reasoning, the reviewer is reduced to a smart linter over diffs â€” the 2023-2024 era approach that failed to catch architectural issues." â€” Claude

Multiple sources confirm: Qodo 2.0 includes "evidence and reasoning that explains how the agent reached its conclusion" in every review finding.

---

## 5. Automation & Tooling

### ðŸŸ¢ CONSENSUS: VS Code v1.109 Is the Canonical Platform

- Runs Claude, Codex, and Copilot simultaneously â€” All 3
- Agent Sessions UI shows all agents in one place â€” ChatGPT
- Both local and cloud execution options â€” Claude
- **`anthropic.claude-code`** extension: subagents, hooks, inline diffs â€” Claude
- **`openai.chatgpt`** extension: three modes (chat-only â†’ full autonomous) â€” Claude

### ðŸŸ¢ CONSENSUS: CI/CD Integration via Official GitHub Actions

| Action | Purpose | Source |
|--------|---------|--------|
| `openai/codex-action@v1` | Code reviews, PR comments, configurable safety | Claude |
| `anthropics/claude-code-action` | PR review, test generation, refactoring proposals | Claude |
| GitHub Agentic Workflows | Agent-neutral CI/CD in markdown (6 pattern categories) | Claude |
| GitHub Agent HQ | Assign issues to Claude, Codex, Copilot simultaneously | Claude |

### ðŸ”µ CONFIRMED: MCP Bridge Tools

| Tool | What It Does | Source |
|------|-------------|--------|
| `pal-mcp-server` | Claude Code spawns Codex/Gemini/OpenRouter subagents | Claude, Gemini |
| `claude-codex-bridge` | Direct MCP communication between agents | Gemini |
| Codex Bridge | Consultation and batch processing | Claude |
| Context Pack | Lightweight anchors, orchestrator renders to markdown pack | Claude |
| `codex mcp-server` | Run Codex CLI as MCP server for Claude Desktop | Claude |

### ðŸ”µ CONFIRMED: Orchestration Frameworks

| Framework | Approach | Source |
|-----------|----------|--------|
| **sudocode** | Context-as-Code, JSONL+SQLite, Kanban visualization | Gemini |
| **ralphex** | Persistent agent loop, `--review` flag for Codex read-only | Gemini |
| **Harbor** | Converts Claude outputs to Codex task format | Gemini |
| **CrewAI / AutoGen / LangGraph** | General multi-agent orchestration | ChatGPT |
| **OpenAI Agents SDK** | Chain PM â†’ Designer â†’ Developer â†’ Tester with traces | Claude |
| **OpenHands** (64K+ GitHub stars) | `AgentDelegateAction`, Docker sandbox, 72% SWE-Bench | Claude |

### ðŸ”µ CONFIRMED: Claude Code Hooks for TDD Enforcement

- **15 hook events, 3 handler types** (command, prompt, agent hooks) â€” Claude
- `PreToolUse` hooks block file edits if no corresponding failing test exists â€” Claude, Gemini
- `PostToolUse` hooks trigger Codex validation sweep on file write â€” Gemini
- **TDD Guard** (open-source): blocks modifications without failing tests (Jest, Vitest, pytest) â€” Claude

### Shell Script Chaining (Practical Automation)

```bash
# Chain Claude non-interactive â†’ Codex headless review
claude -p "implement feature X using TDD" --output-format json
codex exec "review the changes on this branch" --json --output-last-message
```
â€” Claude source

---

## 6. Configuration Best Practices

### ðŸŸ¢ CONSENSUS: File Hierarchy

| File | Purpose | Size Limit | Scope |
|------|---------|-----------|-------|
| **AGENTS.md** | Universal repo standards (cross-platform) | <150 lines (GitHub analysis of 2,500+ files) | All agents |
| **CLAUDE.md** | Claude-specific instructions | <50 lines | Opus only |
| **.codex/config.toml** | Codex operational parameters | Minimal | Codex only |
| **HANDOFF.md** | Ephemeral state transfer | 2,000â€“5,000 tokens | Per-handoff |
| **learnings.md** | Persistent institutional memory | Append-only | Cross-session |

### ðŸŸ¢ CONSENSUS: AGENTS.md as Linux Foundation Standard

- AGENTS.md is now the cross-tool standard, supported by 20+ tools including Codex, Cursor, Jules, Copilot, Gemini CLI â€” Claude
- GitHub analysis: most effective ones stay under 150 lines, put build commands early, use code examples over explanations â€” Claude
- 60,000+ repositories use it â€” Claude

### âšª UNIQUE: ETH Zurich Study (Critical Finding)

> Monolithic auto-generated AGENTS.md files **decrease model performance by ~3%** while **inflating inference costs by 159%**. Irrelevant rules dilute the attention mechanism and lead to hallucinations. â€” Gemini

**Mitigation**: Tiered, path-specific rule injection:
- Directory-scoped `AGENTS.override.md` files (e.g., `services/payments/AGENTS.override.md`)
- Reduces context size by 60â€“80%
- `@docs/architecture.md` syntax for lazy-loading shared knowledge â€” Gemini

### ðŸŸ¢ CONSENSUS: Configuring Codex as Review-Only

| Method | How | Source |
|--------|-----|--------|
| Prompt constraint | "You are a senior engineer reviewing â€” do NOT write new code" | ChatGPT |
| `.codex/config.toml` | `approval_mode = "suggest"` or `ask-for-approval` | Gemini |
| `ralphex --review` | Read-only analysis, structured JSON/Markdown output | Gemini |
| `/review-dirty` script | `codex exec "Review dirty repo changes..."` | ChatGPT |
| CLAUDE.md protocol | "Do not commit without Codex approval" | ChatGPT |

---

## 7. TDD Integration

### ðŸŸ¢ CONSENSUS: TDD Is the #1 Guardrail

> "The simplest guardrail is to make sure your AI uses TDD â€” write a unit test first, then code to pass it." â€” ChatGPT

All three reports agree that TDD is the most effective way to constrain AI agents and prevent drift.

### ðŸŸ¢ CONSENSUS: TDD Theater Is a Major Risk

> "When you ask an AI to generate tests, it almost always starts by analyzing the code. Your tests now validate the implementation, not the intention." â€” Claude

**The failure mode**: AI writes code first, then retrofits trivial tests that pass but don't verify feature intent.

**Mitigations (all sources agree)**:
1. Write tests BEFORE implementation (true TDD, not retrofit)
2. Encode acceptance criteria in natural language in the handoff artifact
3. Have the review agent verify tests against intent, not just that tests pass
4. Use Claude Code hooks / TDD Guard to block edits without failing tests
5. Never let the implementing agent also write the acceptance criteria

### ðŸ”µ CONFIRMED: Tests as Handoff Mechanism

Tests serve as the bridge between agents â€” they're unambiguous, executable, and self-documenting. When Opus writes tests from FIC acceptance criteria, those tests become the spec that Codex validates against.

### âšª UNIQUE: Agent-Specific Test Failure Modes

- **Opus**: Generates tests that pass trivially by heavily mocking internal logic rather than verifying execution paths â€” Gemini
- **Codex**: Over-engineers â€” one case of 80,000 lines of pedantic test suites for a minor feature migration that took 20 hours â€” Gemini

---

## 8. Developer Experience & Productivity

### Productivity Claims

| Claim | Metric | Source | Confidence |
|-------|--------|--------|------------|
| 44 PRs in 5 days | PR throughput | ChatGPT | ðŸ”µ CONFIRMED |
| 93,000 lines in 5 days | Lines shipped | Gemini | âšª UNIQUE |
| 80â€“90% functional success | "Accept all" vibe coding for CRUD/UI | Gemini | âšª UNIQUE |
| Bottleneck is human review speed | Not agent speed | Claude | âšª UNIQUE |

### ðŸŸ¢ CONSENSUS: Sobering Counterpoints

| Finding | Detail | Source |
|---------|--------|--------|
| **METR RCT** | Devs believe 20% faster, actually **19% slower** (246 tasks, mature repos, older models) | Claude |
| **DORA report** | Every 25% AI adoption increase â†’ 1.5% delivery speed dip, 7.2% stability drop | Claude |
| **Stack Overflow 2025** | Trust in AI accuracy dropped from 43% to **33%** YoY; 45% say debugging AI code takes longer than expected | Claude |
| **Columbia DAPLab** | 9 critical failure patterns in 15+ vibe-coded apps; agents prioritize runnable code over correctness | Claude |
| **Anthropic themselves** | "Even frontier Opus 4.5 in a loop falls short of production-quality if only given a high-level prompt" | Claude |

### ðŸ”µ CONFIRMED: Subjective Experience

> "It's closer to being a tech lead than a pair programmer. You sit in the middle, Claude writes, Codex reviews, and you make the judgment calls." â€” Claude (Sakiharu report)

> "The efficiency jumped immediately. Not because the agents got smarter, but because the review discipline became automatic instead of depending on my willpower at 2am." â€” Claude (Sakiharu report)

### âšª UNIQUE: Complementary Context Windows

> "When one agent's context is compressed and loses details, the other agent still remembers. They don't share the same context window, so they don't lose the same information at the same time." â€” Claude

---

## 9. Failure Modes & Anti-Patterns (Ranked by Severity)

### Tier 1: Critical â€” Can Silently Ship Bad Code

| # | Failure Mode | Description | Mitigation | Sources |
|---|-------------|-------------|------------|---------|
| 1 | **Rubber-stamp reviewing** | Codex approves everything; "green checkmarks that mean nothing" | Confidence gates, structured verdicts with evidence, AV checklist | ðŸŸ¢ All 3 |
| 2 | **TDD theater** | Tests validate implementation, not intention; coverage rises but bugs survive | True TDD (tests before code), acceptance criteria in handoff | ðŸŸ¢ All 3 |
| 3 | **Mutual agreement on bad design** | "Two AIs can happily agree on a bad design" â€” human domain judgment required | Human orchestrator is NOT optional | ðŸŸ¢ All 3 |

### Tier 2: Serious â€” Causes Rework or Degradation

| # | Failure Mode | Description | Mitigation | Sources |
|---|-------------|-------------|------------|---------|
| 4 | **Context rot** | Performance degrades beyond 100K tokens; "lost in the middle" phenomenon | Reset context between iterations, keep handoffs lean | ðŸ”µ Claude, Gemini |
| 5 | **Agent ping-pong** | Fixâ†’reviewâ†’new issueâ†’fixâ†’new issue; GPT-4o showed 37.6% MORE vulnerabilities after 5 iterations | Hard iteration limits (3â€“5 rounds), human escalation | ðŸ”µ Claude, ChatGPT |
| 6 | **Agent drift** | Infinite refactoring loops when agents disagree on approach | Human arbiter, explicit acceptance/override in instructions | ðŸ”µ ChatGPT, Gemini |
| 7 | **"Opus Slop"** | Skipped error handling, type loopholes, bypassed edge cases when prioritizing speed | Codex review catches these; explicit error-handling requirements | âšª Gemini |

### Tier 3: Operational â€” Increases Cost/Friction

| # | Failure Mode | Description | Mitigation | Sources |
|---|-------------|-------------|------------|---------|
| 8 | **Configuration fatigue** | Too many .md files; agents partially ignore instructions | Tiered config (AGENTS.md <150 lines), path-specific overrides | ðŸŸ¢ All 3 |
| 9 | **Cost spirals** | 7â€“15Ã— token multiplier from agentic patterns; $200â€“5,600/month | Portfolio approach: cheap models for routine, frontier for complex | ðŸ”µ Claude, Gemini |
| 10 | **Session boundary loss** | Agent session closes, context lost unless explicitly serialized | Handoff artifacts, pomera_notes, git commits as checkpoints | ðŸ”µ ChatGPT, Gemini |
| 11 | **Handoff bloat** | Dumping everything (40K tokens of process to find 3K tokens of insight) | Target 2,000â€“5,000 tokens; diffs + reasoning, not full files | ðŸ”µ Claude, Gemini |

### âšª UNIQUE: Antigravity IDE Rate Limiting

Professional developers report 7â€“10 day lockouts after only 2â€“3 complex Opus 4.6 queries, despite a documented 5-hour refresh window. Evidence of opaque model routing to older pools. Significant cohort migrating to raw Claude Code CLI or Cursor. â€” Gemini

---

## 10. Cost Analysis

### Token Pricing (February 2026)

| Model | Input (per 1M tokens) | Output (per 1M tokens) | Source |
|-------|----------------------|----------------------|--------|
| Claude Opus 4.6 | $5 | $25 | Claude |
| GPT-5.3-Codex | ~$6 | ~$30 | Gemini |

### Real-World Spend

| Scenario | Monthly Cost | Source |
|----------|-------------|--------|
| Subscriptions (Claude Code + planning tools) | $100â€“150 | ChatGPT |
| Heavy dual-agent (API rates, subscriptions) | $200â€“400+ | Claude |
| Extreme case (201 Claude sessions, API rates) | $5,623 | Claude |
| Enterprise dual-agent team | ~$4,000 ($2K Claude + $1.5K OpenAI + $500 monitoring) | Claude |

### Token Multipliers

| Pattern | Multiplier | Source |
|---------|-----------|--------|
| Agent teams (Claude Code) | 7Ã— vs standard | Claude |
| Multi-agent systems | 15Ã— vs single-agent | Claude |
| Subagent pattern vs handoff-with-history | **67% fewer tokens** (9K vs 14K+) | Claude |
| Monolithic AGENTS.md cost inflation | +159% | Gemini (ETH Zurich) |

### ðŸ”µ CONFIRMED: Cost Justified by Productivity

> "While the bill can skyrocket, the speed gains (44 PRs vs months of work) justify it." â€” ChatGPT
> "Not because the agents got smarter, but because the review discipline became automatic." â€” Claude

---

## 11. Actionable Recommendations

### Immediate Setup (Day 1)

1. **Create AGENTS.md** at project root â€” under 150 lines, build commands first, code examples over prose
2. **Create CLAUDE.md** â€” under 50 lines, imports AGENTS.md, adds TDD enforcement
3. **Configure Codex as review-only** â€” `.codex/config.toml` with suggest mode, or `ralphex --review`
4. **Define handoff template** â€” YAML frontmatter + markdown body, target 2,000â€“5,000 tokens
5. **Install TDD Guard** or configure Claude Code hooks to block edits without failing tests

### Workflow Rules

6. **One task per handoff** â€” 87.2% success on single-function vs 19.36% on multi-file tasks
7. **Max 3 review cycles** then escalate to human
8. **Include reasoning in handoffs** â€” not just diffs
9. **Encode acceptance criteria in natural language** â€” separate from test code
10. **Reset context between iterations** â€” don't let sessions accumulate beyond 100K tokens

### Quality Gates

11. **Tests before implementation** â€” true TDD, never retrofit
12. **Human reviews Codex's review** â€” never merge on AI approval alone
13. **Scan for banned patterns** â€” `TODO`, `FIXME`, `NotImplementedError`
14. **Verify tests encode intent, not implementation** â€” adversarial check

### Cost Control

15. **Use subagent pattern** (67% fewer tokens) over growing conversation history
16. **Tiered config** â€” path-specific AGENTS.override.md instead of monolithic files
17. **Cheap models for routine** â€” frontier models only for complex work

---

## 12. Tool Registry

### MCP Bridges & Agent Tools

| Tool | URL/Reference | Purpose |
|------|--------------|---------|
| PAL MCP Server | [github.com/BeehiveInnovations/pal-mcp-server](https://github.com/BeehiveInnovations/pal-mcp-server) | Claude spawns Codex/Gemini subagents |
| claude-codex-bridge | [lobehub.com/mcp/user-claude-codex-bridge](https://lobehub.com/mcp/user-claude-codex-bridge) | Direct MCP communication |
| Codex High-Reasoning Bridge | [mcpmarket.com/tools/skills/codex-high-reasoning-bridge](https://mcpmarket.com/tools/skills/codex-high-reasoning-bridge) | High-reasoning integration |
| Context Pack | (Claude research) | Lightweight anchors for context |
| Codex MCP Server | `codex mcp-server` CLI command | Run Codex as MCP server |

### Orchestration Frameworks

| Tool | URL/Reference | Purpose |
|------|--------------|---------|
| sudocode | [github.com/sudocode-ai/sudocode](https://github.com/sudocode-ai/sudocode) | Context-as-Code, Kanban visualization |
| ralphex | [github.com/umputun/ralphex](https://github.com/umputun/ralphex) | Persistent agent loop, `--review` mode |
| OpenHands | (formerly OpenDevin, 64K+ GitHub stars) | Multi-agent + Docker sandbox |
| Harbor | [github.com/laude-institute/harbor](https://github.com/laude-institute/harbor) | Claudeâ†’Codex format conversion |
| Ruflo | (Claude research) | CLAUDE.md + AGENTS.md dual-mode init |
| TDD Guard | (Claude research) | Blocks edits without failing tests |

### CI/CD Actions

| Action | Purpose |
|--------|---------|
| `openai/codex-action@v1` | Codex CLI in CI, PR review comments |
| `anthropics/claude-code-action` | Claude Code in CI, PR review/test generation |
| GitHub Agentic Workflows | Agent-neutral CI/CD (technical preview) |
| GitHub Agent HQ | Assign issues to multiple agents simultaneously |

### VS Code Extensions

| Extension | Purpose |
|-----------|---------|
| `anthropic.claude-code` | Claude Code with subagents, hooks, inline diffs |
| `openai.chatgpt` | Codex with three autonomy modes |
| VS Code Agent Sessions (v1.109+) | Unified multi-agent management |

### PR Review Tools

| Tool | Speed | Model |
|------|-------|-------|
| Graphite Agent | ~90 seconds (40Ã— faster than human) | Multi-provider |
| Qodo 2.0 / PR-Agent | Multi-agent alignment phase | Custom |
| CodeRabbit | Automated inline review | Multi-provider |
| Traycer | Planâ†’implementâ†’review orchestration | Claude/Codex |

### Planning Tools

| Tool | Purpose |
|------|---------|
| GitHub Spec Kit (v0.1.4) | `spec.md â†’ plan.md â†’ tasks.md` in `.specify/` |
| Traycer | File-level planning for scoped agent calls |

### References

| Source | URL |
|--------|-----|
| ShakaCode Guide | (Claude builds, Codex validates pattern) |
| Builder.io CEO comparison | (Steve Sewell's production comparison) |
| Sakiharu 2-year report | (Detailed first-person dual-agent workflow) |
| ETH Zurich AGENTS.md study | [digitalapplied.com/blog/agents-md-eth-zurich-study](https://www.digitalapplied.com/blog/agents-md-eth-zurich-study-inference-costs-guide) |
| METR RCT | (246 tasks, 16 developers, mature repos) |
| DORA report | (AI adoption impact on delivery speed) |
| Columbia DAPLab | (9 failure patterns in 15+ vibe-coded apps) |
| Addy Osmani workflow | [addyosmani.com/blog/ai-coding-workflow](https://addyosmani.com/blog/ai-coding-workflow/) |
| The Tale of 2 Models (Cordero Core) | [medium.com/@cdcore](https://medium.com/@cdcore/the-tale-of-2-models-opus-4-6-vs-gpt-5-3-codex-129fcb35630f) |
| Lenny's Newsletter (93K lines) | [lennysnewsletter.com](https://www.lennysnewsletter.com/p/claude-opus-46-vs-gpt-53-codex-how) |

---

## Key Contradictions Between Sources

| Topic | ChatGPT | Claude | Gemini |
|-------|---------|--------|--------|
| **Productivity** | 44 PRs in 5 days (positive) | METR: actually 19% slower (cautious) | 93K lines in 5 days (very positive) |
| **Cost justification** | "Justified by speed" | $5,623/month possible, needs portfolio approach | Doubles baseline, ETH Zurich shows 159% waste |
| **Automation maturity** | Scripts + MCP bridges work today | "No universal standard" for handoffs | "Rapidly matured" with sudocode, ralphex |
| **GitHub Actions** | Mentioned briefly | Detailed both actions + Agentic Workflows | Not emphasized |

The Claude report is notably more cautious and evidence-based (citing METR RCT, DORA, Stack Overflow surveys). The Gemini report is most detailed on tooling and configuration. The ChatGPT report is most practical with specific developer quotes and workflow scripts.
